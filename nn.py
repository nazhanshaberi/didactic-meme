# -*- coding: utf-8 -*-
"""NN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cL2gszruyI9CzrH86qqL3IhAYQi8Es7P
"""

# rewrite the code with x=10

import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt
x = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
y = [5, 8, 11, 14, 17, 20, 23, 26, 29, 32]
# Define layer
layer0 = tf.keras.layers.Dense(units=1, input_shape=[1])
model = tf.keras.Sequential([layer0])
# Compile model
model.compile(loss='mean_squared_error',
 optimizer=tf.keras.optimizers.Adam(1))
# Train the model
history = model.fit(x, y, epochs=100, verbose=False)
# Prediction
print('Prediction: {}'.format(model.predict([10])))
# Get weight and bias
weights = layer0.get_weights()
print('weight: {} bias: {}'.format(weights[0], weights[1]))

# x=250

import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt
x = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
y = [5, 8, 11, 14, 17, 20, 23, 26, 29, 32]
# Define layer
layer0 = tf.keras.layers.Dense(units=1, input_shape=[1])
model = tf.keras.Sequential([layer0])
# Compile model
model.compile(loss='mean_squared_error',
 optimizer=tf.keras.optimizers.Adam(1))
# Train the model
history = model.fit(x, y, epochs=100, verbose=False)
# Prediction
print('Prediction: {}'.format(model.predict([250])))
# Get weight and bias
weights = layer0.get_weights()
print('weight: {} bias: {}'.format(weights[0], weights[1]))

#Pridicting X=250 using Adam optimizer but with Mean Absolute Error

import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt
x = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
y = [5, 8, 11, 14, 17, 20, 23, 26, 29, 32]
# Define layer
layer0 = tf.keras.layers.Dense(units=1, input_shape=[1])
model = tf.keras.Sequential([layer0])
# Compile model
model.compile(loss='mean_absolute_error',
 optimizer=tf.keras.optimizers.Adam(1))
# Train the model
history = model.fit(x, y, epochs=100, verbose=False)
# Prediction
print('Prediction: {}'.format(model.predict([250])))
# Get weight and bias
weights = layer0.get_weights()
print('weight: {} bias: {}'.format(weights[0], weights[1]))

"""# Difference Optimizer Testing"""

#using optimizer: Adam

import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt
x = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
y = [5, 8, 11, 14, 17, 20, 23, 26, 29, 32]
# Define layer
layer0 = tf.keras.layers.Dense(units=1, input_shape=[1])
model = tf.keras.Sequential([layer0])
# Compile model
model.compile(loss='mean_squared_error',
 optimizer=tf.keras.optimizers.Adam(1))
# Train the model
history = model.fit(x, y, epochs=100, verbose=False)
# Prediction
print('Prediction: {}'.format(model.predict([250])))
# Get weight and bias
weights = layer0.get_weights()
print('weight: {} bias: {}'.format(weights[0], weights[1]))

#using optimizer: Adagrad

import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt
x = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
y = [5, 8, 11, 14, 17, 20, 23, 26, 29, 32]
# Define layer
layer0 = tf.keras.layers.Dense(units=1, input_shape=[1])
model = tf.keras.Sequential([layer0])
# Compile model
model.compile(loss='mean_squared_error',
 optimizer=tf.keras.optimizers.Adagrad(1))
# Train the model
history = model.fit(x, y, epochs=100, verbose=False)
# Prediction
print('Prediction: {}'.format(model.predict([250])))
# Get weight and bias
weights = layer0.get_weights()
print('weight: {} bias: {}'.format(weights[0], weights[1]))

#using optimizer: RMSprop

import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt
x = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
y = [5, 8, 11, 14, 17, 20, 23, 26, 29, 32]
# Define layer
layer0 = tf.keras.layers.Dense(units=1, input_shape=[1])
model = tf.keras.Sequential([layer0])
# Compile model
model.compile(loss='mean_squared_error',
 optimizer=tf.keras.optimizers.RMSprop(1))
# Train the model
history = model.fit(x, y, epochs=100, verbose=False)
# Prediction
print('Prediction: {}'.format(model.predict([250])))
# Get weight and bias
weights = layer0.get_weights()
print('weight: {} bias: {}'.format(weights[0], weights[1]))

#using optimizer: Adadelta

import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt
x = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
y = [5, 8, 11, 14, 17, 20, 23, 26, 29, 32]
# Define layer
layer0 = tf.keras.layers.Dense(units=1, input_shape=[1])
model = tf.keras.Sequential([layer0])
# Compile model
model.compile(loss='mean_squared_error',
 optimizer=tf.keras.optimizers.Adadelta(1))
# Train the model
history = model.fit(x, y, epochs=100, verbose=False)
# Prediction
print('Prediction: {}'.format(model.predict([250])))
# Get weight and bias
weights = layer0.get_weights()
print('weight: {} bias: {}'.format(weights[0], weights[1]))

"""# Value of the loss magnitude at epoch = 1000

*   loss magnitude has been determined but the value changing everytime.
"""

#Pridicting X=250 using Adagrad optimizer, Mean Absolute Error, Epochs = 1000

import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt
x = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
y = [5, 8, 11, 14, 17, 20, 23, 26, 29, 32]
# Define layer
layer0 = tf.keras.layers.Dense(units=1, input_shape=[1])
model = tf.keras.Sequential([layer0])
# Compile model
model.compile(loss='mean_absolute_error',
 optimizer=tf.keras.optimizers.Adagrad(1))
# Train the model
history = model.fit(x, y, epochs=1000, verbose=False)
Expect = history.history ['loss']
print (history.history ['loss'])
# Display loss magnitude value
print('Loss Magnitude when epochs is 1000:', Expect [999])
# Prediction
print('Prediction: {}'.format(model.predict([250])))
# Get weight and bias
weights = layer0.get_weights()
print('weight: {} bias: {}'.format(weights[0], weights[1]))

"""# Question

*   Find the Python function to return the size and shape of the arrays x_train and y_train.
*   Identify the data type for the variables: layer0, model
"""

import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt
x_train = np.linspace(0, 50, 51)
y_train = np.linspace(5, 155, 51)
y_train = y_train + np.random.normal(0,5,51)
plt.xlabel('x_train')
plt.ylabel('y_train')
plt.scatter(x_train, y_train)
plt.show()
# Define layer
layer0 = tf.keras.layers.Dense(units=1, input_shape=[1])
model = tf.keras.Sequential([layer0])
# Compile model
model.compile(loss='mean_absolute_error',
 optimizer=tf.keras.optimizers.Adagrad(0.1))
# Train the model
history = model.fit(x_train,y_train, epochs=1000, verbose=False)
plt.xlabel('Epoch Number')
plt.ylabel("Loss Magnitude")
plt.plot(history.history['loss'])
plt.show()
# Prediction
print('Prediction: {}'.format(model.predict([250])))
weights = layer0.get_weights()
weight = weights[0][0]
bias = weights[1]
# Get weight and bias
print('weight: {} bias: {}'.format(weight, bias))
#Equation
y_learned = x_train * weight + bias
#Plot the graph
plt.scatter(x_train, y_train, label='Training Data')
plt.plot(x_train, y_learned, color='orangered', label='Fit Line')
plt.legend()
plt.show()
print('')
print('Question 1')
print('The size of x_train:' , x_train.size, 'and the shape of x_train:' , x_train.shape)
print('The size of y_train:' , y_train.size, 'and the shape of y_train:' , y_train.shape)
print('')
print('Question 2')
print('The data type for Layer0:', type (layer0))
print('The data type for Model:', type (model))