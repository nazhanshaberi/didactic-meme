{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Yonko.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyP84uI3OEeVuQUdNdQEyIlp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nazhanshaberi/didactic-meme/blob/main/Yonko.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywJWrQkk2goS"
      },
      "source": [
        "# Difference Epochs Testing (KIV)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p8QZKrbN4rQi"
      },
      "source": [
        "Adam : MSE VS MAE : Epo=1000\n",
        "\n",
        "Discussion:\n",
        "MAE showing high digit with realistic loss magnitude, so next coding will using MAE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6VtdWazV3VTS",
        "outputId": "1fd79a23-9d5d-4b82-d05f-167bf97eb447"
      },
      "source": [
        "#Pridicting X=250 using Adam optimizer, Mean Square Error, Epochs = 1000\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "x = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
        "y = [5, 8, 11, 14, 17, 20, 23, 26, 29, 32]\n",
        "# Define layer\n",
        "layer0 = tf.keras.layers.Dense(units=1, input_shape=[1])\n",
        "model = tf.keras.Sequential([layer0])\n",
        "# Compile model\n",
        "model.compile(loss='mean_squared_error',\n",
        " optimizer=tf.keras.optimizers.Adam(1))\n",
        "# Train the model\n",
        "history = model.fit(x, y, epochs=1000, verbose=False)\n",
        "Expect = history.history ['loss']\n",
        "print (history.history ['loss'])\n",
        "print('Loss Magnitude when epochs is 1000:', Expect [999])\n",
        "# Prediction\n",
        "print('Prediction: {}'.format(model.predict([250])))\n",
        "# Get weight and bias\n",
        "weights = layer0.get_weights()\n",
        "print('weight: {} bias: {}'.format(weights[0], weights[1]))"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[572.412841796875, 314.1087341308594, 135.4765625, 33.97459411621094, 0.5858300924301147, 17.228321075439453, 58.09315872192383, 97.49748229980469, 119.10322570800781, 118.6307144165039, 100.34112548828125, 72.2656021118164, 42.79354476928711, 18.711505889892578, 4.173346996307373, 0.2945697009563446, 5.336217403411865, 15.518157958984375, 26.33218002319336, 33.949302673339844, 36.21654510498047, 32.953529357910156, 25.605504989624023, 16.52398681640625, 8.164125442504883, 2.4077606201171875, 0.13912346959114075, 1.137599229812622, 4.2885894775390625, 8.033246994018555, 10.902254104614258, 11.952881813049316, 10.976961135864258, 8.446091651916504, 5.254889488220215, 2.375058174133301, 0.5385234951972961, 0.04026871919631958, 0.7059062719345093, 2.0155632495880127, 3.3263041973114014, 4.1064958572387695, 4.0993876457214355, 3.3681817054748535, 2.22371244430542, 1.0771183967590332, 0.27953991293907166, 0.006388044450432062, 0.22076371312141418, 0.7186776399612427, 1.2289890050888062, 1.523897647857666, 1.4970852136611938, 1.1844284534454346, 0.7276439666748047, 0.3030264973640442, 0.047689247876405716, 0.01210678182542324, 0.15373364090919495, 0.36838144063949585, 0.5415238738059998, 0.5960098505020142, 0.5173271894454956, 0.3493337035179138, 0.1666267216205597, 0.03843986988067627, 0.0004038410843349993, 0.04484286159276962, 0.130977064371109, 0.207712322473526, 0.23732319474220276, 0.2096126526594162, 0.1418108493089676, 0.06641899049282074, 0.014218183234333992, 0.0007550398586317897, 0.021822134032845497, 0.05859263613820076, 0.08859603106975555, 0.09660263359546661, 0.08043261617422104, 0.049762263894081116, 0.019603505730628967, 0.0023272517137229443, 0.0022323273587971926, 0.014835068956017494, 0.030476903542876244, 0.03983485698699951, 0.03837119787931442, 0.027727171778678894, 0.013862533494830132, 0.0033801812678575516, 0.00013375021808315068, 0.003766791895031929, 0.010598666034638882, 0.01603761501610279, 0.017056236043572426, 0.013461004011332989, 0.007503254804760218, 0.0022979716304689646, 6.218841735972092e-05, 0.0011119095142930746, 0.004019926302134991, 0.006660888437181711, 0.007450277451425791, 0.006088572088629007, 0.0034977495670318604, 0.0011148644844070077, 2.8803804525523447e-05, 0.00045018160017207265, 0.0017471469473093748, 0.00293429521843791, 0.003272437956184149, 0.0026317592710256577, 0.001456718659028411, 0.00041896686889231205, 2.038126240222482e-06, 0.00026249774964526296, 0.0008717277087271214, 0.0013678467366844416, 0.0014408017741516232, 0.001081836991943419, 0.0005355312023311853, 0.00011464682029327378, 7.282150818355149e-06, 0.000189254482393153, 0.0004750796069856733, 0.0006542634218931198, 0.0006187963299453259, 0.00040881038876250386, 0.00016229957691393793, 1.6279887859127484e-05, 2.4758915969869122e-05, 0.00014103506691753864, 0.00026331390836276114, 0.00030552688986063004, 0.00024667897378094494, 0.00013194858911447227, 3.3362623071298e-05, 9.83292579803674e-07, 3.5895496694138274e-05, 9.812924690777436e-05, 0.00013858944294042885, 0.00013106793630868196, 8.447964501101524e-05, 3.1056471925694495e-05, 1.9271758446848253e-06, 7.66322591516655e-06, 3.5219960409449413e-05, 6.033247336745262e-05, 6.495509296655655e-05, 4.764556433656253e-05, 2.1495372493518516e-05, 3.1221316021401435e-06, 1.3143793466952047e-06, 1.2709645488939714e-05, 2.6128876925213262e-05, 3.1218150979839265e-05, 2.505099655536469e-05, 1.2754071576637216e-05, 2.6490326945349807e-06, 1.9443461951595964e-07, 4.8712422540120315e-06, 1.1598135642998386e-05, 1.4892367289576214e-05, 1.2636545761779416e-05, 6.8690510488522705e-06, 1.6735957615310326e-06, 5.062329222482731e-08, 2.0764587134181056e-06, 5.371107363316696e-06, 7.1496192504127976e-06, 6.205911631695926e-06, 3.4378995223960374e-06, 8.686184855832835e-07, 2.7969804250460584e-08, 9.974927479561302e-07, 2.6043994694191497e-06, 3.4650820452952757e-06, 2.9831421670678537e-06, 1.6169899481610628e-06, 3.7838083244423615e-07, 1.8490140973881353e-08, 5.36818163254793e-07, 1.3236973472885438e-06, 1.6956597619355307e-06, 1.395886442878691e-06, 7.041641083560535e-07, 1.324863205809379e-07, 2.0617699547642587e-08, 3.1833778280088154e-07, 6.944724191271234e-07, 8.271237561530143e-07, 6.316441840681364e-07, 2.8038832056154206e-07, 3.390162106597927e-08, 2.7348324493914333e-08, 2.0036307546433818e-07, 3.714007164035138e-07, 3.990232357864443e-07, 2.7283650183562713e-07, 9.935686051676385e-08, 4.381240703565936e-09, 3.179004437470212e-08, 1.2791369385922735e-07, 1.9809651519153704e-07, 1.860802711917131e-07, 1.0837584341061302e-07, 2.835822598967752e-08, 6.764821480764738e-10, 3.0809339079951314e-08, 8.008082374999503e-08, 1.0224304958228458e-07, 8.19969940835108e-08, 3.766444933717139e-08, 4.98876051580055e-09, 3.6300207284512e-09, 2.612687310943329e-08, 4.776456918875738e-08, 4.9740084051563827e-08, 3.2134995109345255e-08, 1.0258554894448935e-08, 1.3935733123027205e-10, 6.005325126068328e-09, 1.8842774451854893e-08, 2.610015670256871e-08, 2.188310155304407e-08, 1.0630742508510593e-08, 1.5516888884903324e-09, 6.681147857179326e-10, 6.331743129806e-09, 1.2077452815617562e-08, 1.285970974151951e-08, 8.251140215520536e-09, 2.5746202947374286e-09, 1.1664269376765457e-11, 1.673197358442735e-09, 5.070842412635557e-09, 6.89342405379989e-09, 5.409060754857364e-09, 2.486490346953474e-09, 2.987007863275437e-10, 3.341710796966879e-10, 1.9923391825216186e-09, 3.3853440051245798e-09, 3.2394382731837368e-09, 1.8002765944657995e-09, 4.340336146047008e-10, 3.8198777474462986e-11, 6.942400543330507e-10, 1.573084773554001e-09, 1.8850414562621154e-09, 1.182820619227698e-09, 4.129333264213386e-10, 7.457856381065664e-12, 2.3521806724602357e-10, 7.34871719032526e-10, 9.765471764566769e-10, 7.7454842539737e-10, 3.4517597113925547e-10, 3.219611383054044e-11, 7.478320046550024e-11, 3.7355221693680107e-10, 5.559058768866976e-10, 4.846242296352443e-10, 2.389470010744077e-10, 2.492015448163354e-11, 2.830802259268239e-11, 1.4026682593204498e-10, 2.744854898928395e-10, 2.560454903655085e-10, 1.3990301972466312e-10, 2.492015448163354e-11, 7.844391802791506e-12, 6.832578947069123e-11, 1.3042153468312279e-10, 1.3042153468312279e-10, 6.832578947069123e-11, 1.921307557495311e-11, 1.8189894306509108e-13, 2.492015448163354e-11, 6.368736787942808e-11, 6.368736787942808e-11, 4.304183745529322e-11, 1.7553247050328125e-11, 1.1141310313558805e-12, 7.457856381065664e-12, 2.830802259268239e-11, 4.333742392725881e-11, 2.830802259268239e-11, 1.0209078374345815e-11, 4.547473576627277e-14, 7.0031090301792e-12, 1.757598580565123e-11, 1.757598580565123e-11, 7.389644451905042e-12, 3.842615028254448e-12, 0.0, 1.546141036382065e-12, 7.025846484459697e-12, 7.025846484459697e-12, 7.025846484459697e-12, 1.546141036382065e-12, 0.0, 1.1823430689367198e-12, 7.0031090301792e-12, 7.0031090301792e-12, 7.0031090301792e-12, 1.1823430689367198e-12, 0.0, 1.546141036382065e-12, 7.025846484459697e-12, 7.025846484459697e-12, 7.025846484459697e-12, 1.546141036382065e-12, 0.0, 1.1823430689367198e-12, 7.0031090301792e-12, 7.0031090301792e-12, 7.0031090301792e-12, 1.1823430689367198e-12, 0.0, 1.546141036382065e-12, 7.025846484459697e-12, 7.025846484459697e-12, 1.546141036382065e-12, 0.0, 1.1823430689367198e-12, 1.1823430689367198e-12, 1.1823430689367198e-12, 1.1823430689367198e-12, 1.1823430689367198e-12, 1.1823430689367198e-12, 1.1823430689367198e-12, 1.1823430689367198e-12, 0.0, 1.546141036382065e-12, 1.546141036382065e-12, 1.546141036382065e-12, 1.546141036382065e-12, 1.546141036382065e-12, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Loss Magnitude when epcohs is 1000: 0.0\n",
            "Prediction: [[755.]]\n",
            "weight: [[3.]] bias: [5.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1psYOJn-4QOq",
        "outputId": "af63488b-c55b-4724-a141-4a5a325f7366"
      },
      "source": [
        "#Pridicting X=250 using Adam optimizer, Mean Absolute Error, Epochs = 1000\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "x = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
        "y = [5, 8, 11, 14, 17, 20, 23, 26, 29, 32]\n",
        "# Define layer\n",
        "layer0 = tf.keras.layers.Dense(units=1, input_shape=[1])\n",
        "model = tf.keras.Sequential([layer0])\n",
        "# Compile model\n",
        "model.compile(loss='mean_absolute_error',\n",
        " optimizer=tf.keras.optimizers.Adam(1))\n",
        "# Train the model\n",
        "history = model.fit(x, y, epochs=1000, verbose=False)\n",
        "Expect = history.history ['loss']\n",
        "print (history.history ['loss'])\n",
        "print('Loss Magnitude when epochs is 1000:', Expect [999])\n",
        "# Prediction\n",
        "print('Prediction: {}'.format(model.predict([250])))\n",
        "# Get weight and bias\n",
        "weights = layer0.get_weights()\n",
        "print('weight: {} bias: {}'.format(weights[0], weights[1]))"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[22.02963638305664, 16.52964210510254, 11.029664993286133, 5.529653072357178, 0.539077639579773, 3.8531899452209473, 5.616976737976074, 5.927752494812012, 5.161700248718262, 3.5826096534729004, 1.3986279964447021, 1.3206727504730225, 2.8325247764587402, 3.3738346099853516, 3.1070377826690674, 2.16051983833313, 0.6373781561851501, 1.3784741163253784, 2.4907422065734863, 2.821833610534668, 2.4744622707366943, 1.535482406616211, 0.07883548736572266, 1.8321549892425537, 2.9251556396484375, 3.2964510917663574, 3.029672622680664, 2.197723865509033, 0.8643642663955688, 0.9144750833511353, 1.9238128662109375, 2.24772047996521, 1.9604641199111938, 1.1277679204940796, 0.24117150902748108, 0.9344558715820312, 1.027885913848877, 0.5407087802886963, 0.46622103452682495, 0.8021346926689148, 0.5367693901062012, 0.2675928473472595, 0.4252413809299469, 0.012353802099823952, 0.24738983809947968, 0.10011234134435654, 0.1415780484676361, 0.21465221047401428, 0.020219754427671432, 0.21200037002563477, 0.18382635712623596, 0.06888022273778915, 0.13769130408763885, 0.28734034299850464, 0.11589173972606659, 0.5961687564849854, 0.6594034433364868, 0.15163221955299377, 0.8687965273857117, 1.2090238332748413, 0.9444149136543274, 0.13635234534740448, 1.1600242853164673, 1.7566722631454468, 1.7244246006011963, 1.12693190574646, 0.08582234382629395, 1.3389848470687866, 1.9924943447113037, 2.0106074810028076, 1.4575623273849487, 0.3910985589027405, 1.1368778944015503, 1.9429031610488892, 2.0998337268829346, 1.6731693744659424, 0.7218026518821716, 0.7049813270568848, 1.4360305070877075, 1.529923677444458, 1.0475802421569824, 0.10151376575231552, 1.2165838479995728, 1.785162329673767, 1.7286937236785889, 1.1101640462875366, 0.031557656824588776, 1.1641557216644287, 1.6287014484405518, 1.4765515327453613, 0.769874632358551, 0.4354166090488434, 0.9503980875015259, 0.8445979952812195, 0.18055219948291779, 0.9854952096939087, 1.466119408607483, 1.3303120136260986, 0.6401316523551941, 0.5486034154891968, 1.0504909753799438, 0.9346296191215515, 0.26317304372787476, 0.907967746257782, 1.3948105573654175, 1.266164779663086, 0.5839283466339111, 0.5962186455726624, 1.0918829441070557, 0.9718631505966187, 0.29804930090904236, 0.8738821744918823, 1.3628190755844116, 1.2373827695846558, 0.5593101978302002, 0.6158608198165894, 1.1083250045776367, 0.9866535067558289, 0.3125351369380951, 0.8585264086723328, 1.3478635549545288, 1.2239269018173218, 0.5484403967857361, 0.6030499935150146, 1.0750049352645874, 0.9357673525810242, 0.24669913947582245, 0.936989426612854, 1.4385380744934082, 1.326409935951233, 0.6621997952461243, 0.49867120385169983, 0.9801607131958008, 0.8504382967948914, 0.1708391159772873, 1.0034496784210205, 1.497450590133667, 1.3794111013412476, 0.7107365131378174, 0.453321635723114, 0.9385326504707336, 0.8129839897155762, 0.14158359169960022, 1.0109764337539673, 1.4828016757965088, 1.3454382419586182, 0.6599838733673096, 0.5185824632644653, 1.0174535512924194, 0.90478515625, 0.24190187454223633, 0.9160149693489075, 1.3966586589813232, 1.2679297924041748, 0.5909284949302673, 0.5793677568435669, 1.0714809894561768, 0.9533966779708862, 0.2862864136695862, 0.8748035430908203, 1.35894775390625, 1.2339975833892822, 0.5610096454620361, 0.6050773859024048, 1.094016194343567, 0.9736732244491577, 0.30511242151260376, 0.8567134737968445, 1.3421027660369873, 1.218841552734375, 0.5479245781898499, 0.6157569885253906, 1.1030865907669067, 0.9818334579467773, 0.3129813075065613, 0.8485924005508423, 1.33428156375885, 1.2118020057678223, 0.542471170425415, 0.5996130704879761, 1.0675480365753174, 0.9292149543762207, 0.24535445868968964, 0.9293721318244934, 1.4272565841674805, 1.3161036968231201, 0.6569287776947021, 0.49536457657814026, 0.9732908010482788, 0.8443984985351562, 0.16947631537914276, 0.9967724084854126, 1.4874732494354248, 1.3702893257141113, 0.7061102986335754, 0.4502696990966797, 0.9322963953018188, 0.8075103759765625, 0.14023932814598083, 1.0051040649414062, 1.4740936756134033, 1.337662696838379, 0.6564470529556274, 0.5149860978126526, 1.010841727256775, 0.8987752795219421, 0.23966941237449646, 0.9116867184638977, 1.3896608352661133, 1.2616857290267944, 0.5884409546852112, 0.5754620432853699, 1.0649057626724243, 0.9474242329597473, 0.2837960124015808, 0.8712862133979797, 1.3529632091522217, 1.2286611795425415, 0.5590613484382629, 0.6012345552444458, 1.0877673625946045, 0.9679969549179077, 0.30263248085975647, 0.8536950945854187, 1.3368160724639893, 1.2141311168670654, 0.5462967157363892, 0.6121021509170532, 1.097240686416626, 0.9765220880508423, 0.3106028139591217, 0.8459270596504211, 1.3295308351516724, 1.2075752019882202, 0.5410802364349365, 0.5962579846382141, 1.062286138534546, 0.9245625734329224, 0.2435310333967209, 0.926413893699646, 1.422277808189392, 1.3115413188934326, 0.654928982257843, 0.4929099678993225, 0.9690153002738953, 0.8406285047531128, 0.16826310753822327, 0.9936229586601257, 1.4825003147125244, 1.3657352924346924, 0.7039541602134705, 0.4482853412628174, 0.928605854511261, 0.8042644262313843, 0.13929958641529083, 1.0020567178726196, 1.4694582223892212, 1.3335247039794922, 0.654649555683136, 0.5128209590911865, 1.0070030689239502, 0.8952873349189758, 0.23831939697265625, 0.9093198776245117, 1.3857734203338623, 1.2582170963287354, 0.5871065855026245, 0.5731504559516907, 1.061070203781128, 0.943942666053772, 0.2823255658149719, 0.8692806363105774, 1.349523663520813, 1.2255940437316895, 0.5579630136489868, 0.5989533066749573, 1.0840795040130615, 0.9646493792533875, 0.3011685907840729, 0.8519115447998047, 1.3336883783340454, 1.2113440036773682, 0.5453400611877441, 0.6099132299423218, 1.0937445163726807, 0.9733458757400513, 0.3091897964477539, 0.8443046808242798, 1.3266493082046509, 1.2050094604492188, 0.540233850479126, 0.5942248702049255, 1.0590934753417969, 0.9217413067817688, 0.24243851006031036, 0.9245786666870117, 1.4192039966583252, 1.3087232112884521, 0.6536906957626343, 0.49139636754989624, 0.9663751721382141, 0.8383026123046875, 0.16752663254737854, 0.9916419982910156, 1.4793813228607178, 1.3628780841827393, 0.7025983333587646, 0.44704121351242065, 0.9262887835502625, 0.8022276163101196, 0.1387164145708084, 1.000115990638733, 1.4665104150772095, 1.330892562866211, 0.6535037755966187, 0.5114404559135437, 1.0045543909072876, 0.8930639028549194, 0.23746681213378906, 0.9077885746955872, 1.3832632303237915, 1.2559760808944702, 0.5862411856651306, 0.5716606378555298, 1.058593988418579, 0.9416958093643188, 0.28138503432273865, 0.8679607510566711, 1.3472692966461182, 1.223583698272705, 0.557239294052124, 0.5974639654159546, 1.0816682577133179, 0.9624603390693665, 0.3002181947231293, 0.8507221341133118, 1.3316116333007812, 1.2094924449920654, 0.5446999073028564, 0.6084663271903992, 1.0914294719696045, 0.9712463617324829, 0.3082629144191742, 0.8432065844535828, 1.3247121572494507, 1.2032829523086548, 0.5396597981452942, 0.5928685665130615, 1.0569578409194946, 0.9198562502861023, 0.24171657860279083, 0.923326313495636, 1.4171133041381836, 1.3068047761917114, 0.6528428196907043, 0.4903785288333893, 0.9645923376083374, 0.8367320895195007, 0.16703777015209198, 0.990278422832489, 1.4772412776947021, 1.3609168529510498, 0.7016647458076477, 0.44619297981262207, 0.924704909324646, 0.8008367419242859, 0.1383253037929535, 0.9987686276435852, 1.4644687175750732, 1.3290693759918213, 0.6527096033096313, 0.51048743724823, 1.002860188484192, 0.8915254473686218, 0.23687967658042908, 0.9067174792289734, 1.3815109729766846, 1.2544115781784058, 0.5856372117996216, 0.5706180334091187, 1.0568616390228271, 0.9401238560676575, 0.280729204416275, 0.867030143737793, 1.3456827402114868, 1.2221683263778687, 0.5567303895950317, 0.5964130163192749, 1.079965591430664, 0.9609153866767883, 0.29954853653907776, 0.8498771786689758, 1.3301388025283813, 1.2081794738769531, 0.5442480444908142, 0.607435405254364, 1.089780569076538, 0.9697479009628296, 0.30760160088539124, 0.8424240350723267, 1.3233280181884766, 1.202049970626831, 0.5392526388168335, 0.5918935537338257, 1.0554230213165283, 0.918501079082489, 0.2411983460187912, 0.9224244952201843, 1.4156090021133423, 1.3054254055023193, 0.6522353887557983, 0.4896388649940491, 0.9633002281188965, 0.8355954885482788, 0.16668367385864258, 0.989291787147522, 1.4756906032562256, 1.3594958782196045, 0.7009919881820679, 0.44557079672813416, 0.9235458374023438, 0.7998172044754028, 0.13803324103355408, 0.9977906942367554, 1.4629838466644287, 1.3277437686920166, 0.6521360278129578, 0.5097820162773132, 1.0016145706176758, 0.8903924226760864, 0.23644642531871796, 0.905931830406189, 1.3802261352539062, 1.2532649040222168, 0.585197925567627, 0.5698450207710266, 1.0555802583694458, 0.9389599561691284, 0.2802424430847168, 0.8663433194160461, 1.3445098400115967, 1.2211215496063232, 0.5563560724258423, 0.5956306457519531, 1.078700304031372, 0.9597671627998352, 0.2990521490573883, 0.8492473363876343, 1.3290412425994873, 1.2072007656097412, 0.5439102053642273, 0.6066654324531555, 1.0885493755340576, 0.968631386756897, 0.3071104884147644, 0.8418331146240234, 1.3222877979278564, 1.2011222839355469, 0.5389456152915955, 0.5911634564399719, 1.0542733669281006, 0.917486310005188, 0.240813210606575, 0.9217397570610046, 1.4144699573516846, 1.3043807744979858, 0.6517717838287354, 0.48908501863479614, 0.962329089641571, 0.8347402811050415, 0.16642136871814728, 0.9885374903678894, 1.4745099544525146, 1.3584140539169312, 0.7004756927490234, 0.44510477781295776, 0.9226737022399902, 0.7990524172782898, 0.1378212869167328, 0.9970362782478333, 1.461843490600586, 1.3267242908477783, 0.6516897082328796, 0.50925213098526, 1.000671625137329, 0.8895371556282043, 0.23612423241138458, 0.9053226709365845, 1.37923264503479, 1.2523765563964844, 0.5848501324653625, 0.5692640542984009, 1.0546088218688965, 0.9380801916122437, 0.27988201379776, 0.8658021092414856, 1.3435962200164795, 1.2203054428100586, 0.5560566186904907, 0.595039963722229, 1.0777380466461182, 0.9588943719863892, 0.29868263006210327, 0.8487478494644165, 1.3281805515289307, 1.2064316272735596, 0.5436376333236694, 0.6060827970504761, 1.087610125541687, 0.9677799940109253, 0.3067437708377838, 0.8413610458374023, 1.321467638015747, 1.2003905773162842, 0.5386966466903687, 0.5906080007553101, 1.0533902645111084, 0.9167083501815796, 0.24052420258522034, 0.9211975932121277, 1.4135749340057373, 1.3035589456558228, 0.6514052152633667, 0.48865970969200134, 0.9615775942802429, 0.8340808749198914, 0.16622433066368103, 0.987940788269043, 1.4735798835754395, 1.3575613498687744, 0.700066089630127, 0.4447435736656189, 0.9219926595687866, 0.7984558343887329, 0.1376577913761139, 0.9964405298233032, 1.460944652557373, 1.3259212970733643, 0.651337206363678, 0.5088371634483337, 0.9999296069145203, 0.8888646960258484, 0.23587194085121155, 0.9048416018486023, 1.3784501552581787, 1.2516789436340332, 0.5845796465873718, 0.5687991976737976, 1.0538365840911865, 0.9373793601989746, 0.2795921862125397, 0.8653791546821594, 1.3428771495819092, 1.2196645736694336, 0.5558243989944458, 0.5945665240287781, 1.076968789100647, 0.9581979513168335, 0.298383891582489, 0.8483561277389526, 1.3275017738342285, 1.2058262825012207, 0.5434252619743347, 0.6056138277053833, 1.0868566036224365, 0.9670965075492859, 0.30644670128822327, 0.8409910202026367, 1.3208191394805908, 1.1998116970062256, 0.5385023355484009, 0.5901598930358887, 1.0526827573776245, 0.9160841107368469, 0.24029111862182617, 0.9207647442817688, 1.412858247756958, 1.3029015064239502, 0.6511112451553345, 0.4883178174495697, 0.9609745144844055, 0.8335503339767456, 0.16606450080871582, 0.9874626398086548, 1.4728331565856934, 1.3568775653839111, 0.6997390985488892, 0.4444504678249359, 0.921444296836853, 0.7979735732078552, 0.13752417266368866, 0.9959623217582703, 1.4602229595184326, 1.3252754211425781, 0.6510540246963501, 0.508501410484314, 0.9993323087692261, 0.8883230090141296, 0.23566922545433044, 0.9044505953788757, 1.377814531326294, 1.2511100769042969, 0.584357738494873, 0.5684295892715454, 1.0532171726226807, 0.9368179440498352, 0.27936235070228577, 0.8650321960449219, 1.3422905206680298, 1.2191389799118042, 0.5556318759918213, 0.5941855907440186, 1.076348900794983, 0.9576363563537598, 0.2981460690498352, 0.8480318188667297, 1.3269438743591309, 1.2053279876708984, 0.5432490706443787, 0.6052362322807312, 1.0862481594085693, 0.9665452241897583, 0.30620989203453064, 0.8406833410263062, 1.3202848434448242, 1.1993350982666016, 0.5383408069610596, 0.5897979140281677, 1.0521080493927002, 0.9155782461166382, 0.24010467529296875, 0.9204075932502747, 1.4122705459594727, 1.3023607730865479, 0.6508668065071106, 0.48804205656051636, 0.9604846835136414, 0.8331207036972046, 0.16593924164772034, 0.9870666265487671, 1.472218632698059, 1.3563125133514404, 0.6994665861129761, 0.44421666860580444, 0.9209993481636047, 0.7975839376449585, 0.13741913437843323, 0.9955650568008423, 1.4596242904663086, 1.3247407674789429, 0.6508178114891052, 0.5082303285598755, 0.9988445043563843, 0.8878813982009888, 0.23550787568092346, 0.9041258692741394, 1.377288579940796, 1.2506401538848877, 0.5841715335845947, 0.5681257247924805, 1.0527071952819824, 0.936355471611023, 0.2791748046875, 0.8647422790527344, 1.3418033123016357, 1.2187036275863647, 0.5554698705673218, 0.5938736200332642, 1.0758390426635742, 0.9571741223335266, 0.297952800989151, 0.8477621078491211, 1.3264820575714111, 1.2049155235290527, 0.54310142993927, 0.6049265265464783, 1.085746169090271, 0.9660905599594116, 0.3060162663459778, 0.8404258489608765, 1.3198394775390625, 1.1989368200302124, 0.5382040739059448, 0.5895006060600281, 1.0516338348388672, 0.9151614904403687, 0.23995284736156464, 0.9201076626777649, 1.411778450012207, 1.3019096851348877, 0.6506631970405579, 0.48781412839889526, 0.9600796699523926, 0.8327642679214478, 0.16583600640296936, 0.9867356419563293, 1.4717044830322266, 1.3558406829833984, 0.6992382407188416, 0.4440210461616516, 0.9206289052963257, 0.7972597479820251, 0.1373327225446701, 0.9952316284179688, 1.4591237306594849, 1.3242928981781006, 0.6506187915802002, 0.508003830909729, 0.9984378814697266, 0.8875131607055664, 0.23537369072437286, 0.9038524627685547, 1.3768470287322998, 1.250244379043579, 0.5840141177177429, 0.5678737163543701, 1.052283525466919, 0.9359716176986694, 0.27902063727378845, 0.8644973635673523, 1.3413927555084229, 1.218336820602417, 0.5553333163261414, 0.5936138033866882, 1.0754129886627197, 0.9567884206771851, 0.29779085516929626, 0.8475345373153687, 1.3260917663574219, 1.2045660018920898, 0.5429761409759521, 0.6046656966209412, 1.0853248834609985, 0.9657096862792969, 0.305854469537735, 0.8402080535888672, 1.3194636106491089, 1.1986005306243896, 0.5380877256393433, 0.5892521142959595, 1.0512360334396362, 0.9148114919662476, 0.23982767760753632, 0.9198524355888367, 1.4113619327545166, 1.3015258312225342, 0.6504883170127869, 0.48762303590774536, 0.9597375988960266, 0.8324640393257141, 0.16574974358081818, 0.986453652381897, 1.4712682962417603, 1.3554408550262451, 0.699043333530426, 0.4438591003417969, 0.9203182458877563, 0.7969878911972046, 0.13726286590099335, 0.9949467778205872, 1.458698034286499, 1.3239104747772217, 0.6504490375518799, 0.5078128576278687, 0.9980939626693726, 0.8872009515762329, 0.2352580577135086, 0.9036210179328918, 1.3764736652374268, 1.2499115467071533, 0.5838828086853027, 0.567657470703125, 1.0519205331802368, 0.9356434941291809, 0.27888792753219604, 0.8642913699150085, 1.3410459756851196, 1.218027114868164, 0.5552189946174622, 0.5933918356895447, 1.0750499963760376, 0.9564592242240906, 0.29765287041664124, 0.8473405838012695, 1.3257594108581543, 1.2042694091796875, 0.5428684949874878, 0.6044450402259827, 1.0849679708480835, 0.965384840965271, 0.30571669340133667, 0.8400219082832336, 1.3191430568695068, 1.1983144283294678, 0.5379897952079773, 0.5890394449234009, 1.050897479057312, 0.9145132303237915, 0.23971915245056152, 0.9196359515190125, 1.4110071659088135, 1.3012003898620605, 0.6503405570983887, 0.4874599575996399, 0.9594465494155884, 0.8322089910507202, 0.16567692160606384, 0.9862135648727417, 1.470895528793335, 1.3550989627838135, 0.6988775730133057, 0.4437185823917389, 0.9200519323348999, 0.7967546582221985, 0.13720063865184784, 0.9947042465209961, 1.4583337306976318, 1.3235857486724854, 0.6503040790557861, 0.5076507925987244, 0.997800350189209, 0.8869350552558899, 0.23516277968883514, 0.9034204483032227, 1.3761507272720337, 1.2496213912963867, 0.5837669968605042, 0.5674749612808228, 1.051613211631775, 0.9353653192520142, 0.27877768874168396, 0.8641113042831421, 1.340745210647583, 1.2177578210830688, 0.5551174879074097, 0.5932033658027649, 1.0747406482696533, 0.9561792612075806, 0.297537624835968, 0.8471714854240417, 1.3254715204238892, 1.2040115594863892, 0.542773962020874, 0.6042564511299133, 1.084660530090332, 0.9651073217391968, 0.30559903383255005, 0.8398616909980774, 1.3188663721084595, 1.1980669498443604, 0.5379047393798828, 0.5888571739196777, 1.0506060123443604, 0.9142572283744812, 0.23962755501270294, 0.919447124004364, 1.4106988906860352, 1.3009164333343506, 0.650209903717041, 0.4873220920562744, 0.9591987729072571, 0.831991970539093, 0.1656174212694168, 0.9860023260116577, 1.4705712795257568, 1.3548002243041992, 0.6987306475639343, 0.4436013698577881, 0.9198247790336609, 0.7965565919876099, 0.13715043663978577, 0.994490921497345, 1.4580157995224, 1.3233006000518799, 0.6501750349998474, 0.5075131058692932, 0.997550368309021, 0.8867085576057434, 0.2350822389125824, 0.9032440185546875, 1.3758690357208252, 1.2493687868118286, 0.5836637020111084, 0.5673200488090515, 1.051349401473999, 0.9351266622543335, 0.2786830961704254, 0.86395263671875, 1.3404817581176758, 1.217522144317627, 0.5550273656845093, 0.5930417776107788, 1.0744740962982178, 0.9559386968612671, 0.29743894934654236, 0.8470228314399719, 1.3252193927764893, 1.2037861347198486, 0.5426895022392273, 0.604096531867981, 1.0843985080718994, 0.9648706316947937, 0.305502325296402, 0.839716911315918, 1.318619966506958, 1.1978455781936646, 0.5378260016441345, 0.5887035131454468, 1.0503565073013306, 0.9140375256538391, 0.2395515888929367, 0.9192811846733093, 1.4104286432266235, 1.3006680011749268, 0.6500954627990723, 0.4872024655342102, 0.9589828252792358, 0.8318036794662476, 0.1655658781528473, 0.9858188629150391, 1.4702885150909424, 1.354541301727295, 0.6986036896705627, 0.44349750876426697, 0.919624924659729, 0.7963823676109314, 0.1371038854122162, 0.9943070411682129, 1.4577406644821167, 1.3230539560317993, 0.6500651240348816, 0.5073907375335693, 0.9973293542861938, 0.886509120464325, 0.23501114547252655, 0.903091549873352, 1.375624418258667, 1.2491505146026611, 0.5835766792297363, 0.567182183265686, 1.051116943359375, 0.9349168539047241, 0.2786000669002533, 0.8638151288032532, 1.340253233909607, 1.2173168659210205, 0.5549494624137878, 0.5928999781608582, 1.07423996925354, 0.9557271003723145, 0.29735103249549866, 0.8468948602676392, 1.3250024318695068, 1.2035919427871704, 0.5426198244094849, 0.6039525270462036, 1.084164023399353, 0.9646579027175903, 0.3054116666316986, 0.8395967483520508, 1.3184120655059814, 1.1976605653762817, 0.5377636551856995, 0.5885627865791321, 1.0501320362091064, 0.9138407707214355, 0.2394792139530182, 0.9191402196884155, 1.4101969003677368, 1.3004547357559204, 0.6499994397163391, 0.48709383606910706, 0.958789050579071, 0.8316329717636108, 0.1655154675245285, 0.9856621026992798, 1.4700443744659424, 1.35431706905365, 0.6984957456588745, 0.4434043765068054, 0.9194480776786804, 0.7962263226509094, 0.13706235587596893, 0.9941471219062805, 1.457500696182251, 1.322838544845581, 0.6499691605567932, 0.5072818994522095, 0.9971336126327515, 0.8863315582275391, 0.23494596779346466, 0.9029596447944641, 1.3754117488861084, 1.2489604949951172, 0.583501398563385, 0.5670591592788696, 1.0509111881256104, 0.9347308874130249, 0.2785259783267975, 0.8636963963508606]\n",
            "Loss Magnitude when epcohs is 1000: 0.8636963963508606\n",
            "Prediction: [[820.5486]]\n",
            "weight: [[3.261542]] bias: [5.163114]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqYpgCy-5hLi"
      },
      "source": [
        "Different between Epochs (With loss magnitude)\n",
        "\n",
        "\n",
        "*   Higher epoch not nessesaryly giving higher value\n",
        "*   500>200>1000\n",
        "*   No correlation can be made\n",
        "*   Need further study on what (Expect[]) mean\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ESFuq6mO2pV5",
        "outputId": "7aca64a6-8f89-48e1-d352-46954c482db7"
      },
      "source": [
        "#Pridicting X=250 using Adam optimizer, Mean Absolute Error, Epochs = 200\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "x = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
        "y = [5, 8, 11, 14, 17, 20, 23, 26, 29, 32]\n",
        "# Define layer\n",
        "layer0 = tf.keras.layers.Dense(units=1, input_shape=[1])\n",
        "model = tf.keras.Sequential([layer0])\n",
        "# Compile model\n",
        "model.compile(loss='mean_absolute_error',\n",
        " optimizer=tf.keras.optimizers.Adam(1))\n",
        "# Train the model\n",
        "history = model.fit(x, y, epochs=200, verbose=False)\n",
        "Expect = history.history ['loss']\n",
        "print (history.history ['loss'])\n",
        "print('Loss Magnitude when epochs is 100:', Expect [99])\n",
        "# Prediction\n",
        "print('Prediction: {}'.format(model.predict([250])))\n",
        "# Get weight and bias\n",
        "weights = layer0.get_weights()\n",
        "print('weight: {} bias: {}'.format(weights[0], weights[1]))"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[19.184234619140625, 13.684240341186523, 8.18426513671875, 2.6842522621154785, 3.04614520072937, 5.9096174240112305, 7.0754876136779785, 7.035694122314453, 6.017672061920166, 4.327362060546875, 2.0939419269561768, 0.7396217584609985, 2.27948260307312, 2.852224826812744, 2.618837833404541, 1.706707239151001, 0.2182365357875824, 1.7631340026855469, 2.8491618633270264, 3.1607789993286133, 2.799509286880493, 1.8512446880340576, 0.3891412615776062, 1.5241243839263916, 2.622267961502075, 3.0009922981262207, 2.7434520721435547, 1.9221702814102173, 0.6005935668945312, 1.1656004190444946, 2.1659414768218994, 2.4839539527893066, 2.1934401988983154, 1.3597304821014404, 0.07000484317541122, 1.1718639135360718, 1.683749794960022, 1.568877935409546, 0.8931125402450562, 0.28484171628952026, 0.7726935744285583, 0.6420786380767822, 0.04299597814679146, 0.09171958267688751, 0.4304211735725403, 0.3374233841896057, 0.2954443097114563, 0.29762911796569824, 0.26562994718551636, 0.2268870323896408, 0.3692665994167328, 0.34300559759140015, 0.24247193336486816, 0.20754680037498474, 0.38492149114608765, 0.3571181297302246, 0.22822146117687225, 0.19470353424549103, 0.397736132144928, 0.3881894648075104, 0.15893855690956116, 0.09061451256275177, 0.530222475528717, 0.5295823216438293, 0.03606419637799263, 0.03744683414697647, 0.4962156414985657, 0.41600194573402405, 0.21550865471363068, 0.22449307143688202, 0.3371542990207672, 0.29671579599380493, 0.2921099066734314, 0.25152388215065, 0.34555965662002563, 0.3419448435306549, 0.21880587935447693, 0.16456560790538788, 0.4428893029689789, 0.4309292435646057, 0.1454392373561859, 0.12186188995838165, 0.4064871370792389, 0.31495755910873413, 0.3262794613838196, 0.34448257088661194, 0.20450706779956818, 0.15749716758728027, 0.3934219479560852, 0.3215600848197937, 0.30220919847488403, 0.30446892976760864, 0.252401202917099, 0.199832484126091, 0.3950716555118561, 0.3669675290584564, 0.2203458845615387, 0.2050950527191162, 0.34360867738723755, 0.27843618392944336, 0.33894914388656616, 0.3358291685581207, 0.2368146926164627, 0.20789280533790588, 0.33243370056152344, 0.24644842743873596, 0.38989195227622986, 0.4036060869693756, 0.14285330474376678, 0.07887830585241318, 0.5220757722854614, 0.5014375448226929, 0.07754234969615936, 0.057764291763305664, 0.5171130895614624, 0.47585147619247437, 0.1197691410779953, 0.09732814133167267, 0.48120611906051636, 0.443560928106308, 0.14848990738391876, 0.12315640598535538, 0.4602126181125641, 0.44441747665405273, 0.10647635161876678, 0.043187569826841354, 0.5718525648117065, 0.5672856569290161, 0.009651899337768555, 0.3722517490386963, 0.1519535481929779, 0.6061612963676453, 0.7285875082015991, 0.28899627923965454, 0.6642524600028992, 0.9534047245979309, 0.6539789438247681, 0.19261164963245392, 0.42417293787002563, 0.09542226791381836, 0.7113097906112671, 0.8709700703620911, 0.45777949690818787, 0.4591129422187805, 0.7213117480278015, 0.3972910940647125, 0.4731341004371643, 0.6917285323143005, 0.36662253737449646, 0.49155670404434204, 0.6869239211082458, 0.302776038646698, 0.6027930974960327, 0.8578716516494751, 0.5276613831520081, 0.32916706800460815, 0.5406209230422974, 0.17511501908302307, 0.6994272470474243, 0.92351895570755, 0.5657073855400085, 0.3156866431236267, 0.5495096445083618, 0.20065435767173767, 0.6724863648414612, 0.8990814089775085, 0.5439159274101257, 0.33471378684043884, 0.5664383172988892, 0.21607312560081482, 0.6580575108528137, 0.8859159350395203, 0.5322508215904236, 0.3446737825870514, 0.5752232074737549, 0.2241503745317459, 0.6502724885940552, 0.8787410855293274, 0.5259628295898438, 0.34983405470848083, 0.5797020196914673, 0.22834272682666779, 0.6460193395614624, 0.874755859375, 0.5225329995155334, 0.3524552285671234, 0.5819084644317627, 0.2304791957139969, 0.643649160861969, 0.872475266456604, 0.5206269025802612, 0.3537377417087555, 0.5829190015792847, 0.23152880370616913, 0.6422860026359558, 0.8711113929748535, 0.5195357799530029, 0.35431379079818726, 0.5833050012588501, 0.2320081740617752]\n",
            "Loss Magnitude when epochs is 100: 0.27843618392944336\n",
            "Prediction: [[783.3489]]\n",
            "weight: [[3.1128612]] bias: [5.133588]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ncDVc2V3M-K",
        "outputId": "817e24cc-2ef6-4c9b-85c6-f5f5368861c2"
      },
      "source": [
        "#Pridicting X=250 using Adam optimizer, Mean Absolute Error, Epochs = 500\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "x = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
        "y = [5, 8, 11, 14, 17, 20, 23, 26, 29, 32]\n",
        "# Define layer\n",
        "layer0 = tf.keras.layers.Dense(units=1, input_shape=[1])\n",
        "model = tf.keras.Sequential([layer0])\n",
        "# Compile model\n",
        "model.compile(loss='mean_absolute_error',\n",
        " optimizer=tf.keras.optimizers.Adam(1))\n",
        "# Train the model\n",
        "history = model.fit(x, y, epochs=500, verbose=False)\n",
        "Expect = history.history ['loss']\n",
        "print (history.history ['loss'])\n",
        "print('Loss Magnitude when epcohs is 500:', Expect [499])\n",
        "# Prediction\n",
        "print('Prediction: {}'.format(model.predict([250])))\n",
        "# Get weight and bias\n",
        "weights = layer0.get_weights()\n",
        "print('weight: {} bias: {}'.format(weights[0], weights[1]))"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[13.857441902160645, 8.357446670532227, 2.8574700355529785, 3.2362098693847656, 5.471955299377441, 6.030230522155762, 5.348663806915283, 3.84639310836792, 1.8976929187774658, 1.3207720518112183, 2.63376522064209, 2.9099204540252686, 2.3473126888275146, 1.0956518650054932, 1.0427913665771484, 1.8896162509918213, 2.143477439880371, 1.7248408794403076, 0.731370747089386, 0.8056480288505554, 1.4583839178085327, 1.3886768817901611, 0.7148382663726807, 0.5412022471427917, 1.0095841884613037, 0.8080459833145142, 0.3292083144187927, 0.6168943643569946, 0.539668083190918, 0.2346571981906891, 0.486832857131958, 0.14237861335277557, 0.3975306451320648, 0.24937334656715393, 0.4864657521247864, 0.5509917140007019, 0.14738225936889648, 0.7811493873596191, 0.8562677502632141, 0.313687801361084, 0.8130823373794556, 1.1950809955596924, 0.9774848222732544, 0.2518947720527649, 0.9840450286865234, 1.4288685321807861, 1.2277047634124756, 0.4484651982784271, 0.8481996655464172, 1.4162994623184204, 1.331752896308899, 0.6624613404273987, 0.5717141628265381, 1.0952367782592773, 1.0207916498184204, 0.4127197265625, 0.7195743322372437, 1.111684799194336, 0.8744966387748718, 0.07273030281066895, 1.23557448387146, 1.8242534399032593, 1.767009973526001, 1.129974603652954, 0.11105994880199432, 0.7072137594223022, 0.7505913376808167, 0.21223917603492737, 0.8475807309150696, 1.209061622619629, 0.9494704008102417, 0.1527656614780426, 1.1215250492095947, 1.6647682189941406, 1.569882869720459, 0.9017826318740845, 0.29106783866882324, 0.7865129709243774, 0.6592508554458618, 0.036336421966552734, 0.08122467994689941, 0.45858946442604065, 0.36428046226501465, 0.29987460374832153, 0.318284809589386, 0.24365076422691345, 0.17093010246753693, 0.4722100794315338, 0.4733158051967621, 0.10267658531665802, 0.04410839080810547, 0.5848711133003235, 0.5746678113937378, 0.021538591012358665, 0.6720653176307678, 0.6893116235733032, 0.14024510979652405, 0.8920329809188843, 1.230783462524414, 0.9579421877861023, 0.13530273735523224, 1.1815736293792725, 1.7895333766937256, 1.760117530822754, 1.1576554775238037, 0.06959591060876846, 1.4151365756988525, 2.1478047370910645, 2.2308664321899414, 1.7298485040664673, 0.7036769390106201, 0.7946516275405884, 1.5675541162490845, 1.688136339187622, 1.22214674949646, 0.22872047126293182, 1.2389601469039917, 1.9856088161468506, 2.0838351249694824, 1.5989488363265991, 0.6161649823188782, 0.8696562051773071, 1.6097615957260132, 1.7030179500579834, 1.214539647102356, 0.20289692282676697, 1.2792249917984009, 2.040893793106079, 2.1545822620391846, 1.6854839324951172, 0.7027925848960876, 0.7510055899620056, 1.4785106182098389, 1.5622403621673584, 1.0669353008270264, 0.05247006565332413, 1.2304480075836182, 1.811700463294983, 1.7633111476898193, 1.1486016511917114, 0.024536514654755592, 1.5576385259628296, 2.410536289215088, 2.6074483394622803, 2.214310884475708, 1.290442705154419, 0.11079178005456924, 0.8017172813415527, 0.8536975979804993, 0.33093857765197754, 0.70931077003479, 1.0968201160430908, 0.876703143119812, 0.13504505157470703, 1.0868160724639893, 1.5940449237823486, 1.481353998184204, 0.8110146522521973, 0.360948383808136, 0.8467720150947571, 0.715359628200531, 0.05632462352514267, 0.4590579867362976, 0.34909528493881226, 0.319610595703125, 0.3514665961265564, 0.18956223130226135, 0.10679326206445694, 0.5405725240707397, 0.5722501873970032, 0.09161376953125, 0.7791611552238464, 0.9544878005981445, 0.5421150922775269, 0.39892736077308655, 0.6757746934890747, 0.3551221489906311, 0.521493136882782, 0.739313006401062, 0.40594977140426636, 0.4363188147544861, 0.6022430062294006, 0.18192815780639648, 0.7657588124275208, 1.0491079092025757, 0.7348085641860962, 0.11713609844446182, 0.3147229254245758, 0.07636933028697968, 0.14781150221824646, 0.2115069329738617, 0.05743760988116264, 0.17541703581809998, 0.1994381844997406, 0.054901886731386185, 0.20208898186683655, 0.15994390845298767, 0.06859378516674042, 0.29818883538246155, 0.05906252935528755, 0.7251563668251038, 0.861889660358429, 0.4161113202571869, 0.5537169575691223, 0.8577922582626343, 0.5629107356071472, 0.2831030488014221, 0.4771544933319092, 0.12630271911621094, 0.7213399410247803, 0.9016401171684265, 0.49546629190444946, 0.43833470344543457, 0.7103708386421204, 0.38702908158302307, 0.49480724334716797, 0.7145230770111084, 0.3882603645324707, 0.45661598443984985, 0.619854211807251, 0.19868287444114685, 0.7482680082321167, 1.0324945449829102, 0.7204688787460327, 0.12799802422523499, 0.3239073157310486, 0.06729531288146973, 0.15417051315307617, 0.20390553772449493, 0.06053466722369194, 0.16923241317272186, 0.20415930449962616, 0.052810002118349075, 0.12289337813854218, 0.31005367636680603, 0.13173647224903107, 0.5757361650466919, 0.6440669894218445, 0.14242620766162872, 0.8646400570869446, 1.1985852718353271, 0.931268572807312, 0.12299208343029022, 1.17198646068573, 1.7697337865829468, 1.740156888961792, 1.1461626291275024, 0.06694474071264267, 1.389613151550293, 2.1123576164245605, 2.1951873302459717, 1.7022645473480225, 0.691323459148407, 0.785682737827301, 1.5475717782974243, 1.6660335063934326, 1.2055752277374268, 0.22424474358558655, 1.2257252931594849, 1.9637048244476318, 2.0610623359680176, 1.5820167064666748, 0.6110190153121948, 0.8586532473564148, 1.5906658172607422, 1.6829421520233154, 1.1996091604232788, 0.1983700692653656, 1.2688499689102173, 2.023024082183838, 2.135615110397339, 1.6709272861480713, 0.6975212097167969, 0.7433834075927734, 1.4645296335220337, 1.547670602798462, 1.0567458868026733, 0.05133800581097603, 1.2196906805038452, 1.795374870300293, 1.747237205505371, 1.1377995014190674, 0.023326730355620384, 1.5455554723739624, 2.3914742469787598, 2.586876153945923, 2.1969497203826904, 1.2803575992584229, 0.11011095345020294, 0.7958154678344727, 0.8473722338676453, 0.32832759618759155, 0.7045220136642456, 1.0893634557724, 0.8706958889961243, 0.13366350531578064, 1.0796725749969482, 1.5836740732192993, 1.4719213247299194, 0.8061122894287109, 0.35822978615760803, 0.8408941030502319, 0.7101791501045227, 0.05635180324316025, 0.4566589295864105, 0.3471161425113678, 0.3170311450958252, 0.34884268045425415, 0.18832159042358398, 0.10597648471593857, 0.5370679497718811, 0.5685662627220154, 0.09051704406738281, 0.7736304998397827, 0.9480047225952148, 0.5387829542160034, 0.3955531716346741, 0.6703323125839233, 0.35163813829421997, 0.5194472074508667, 0.7359297871589661, 0.4045848250389099, 0.4322967529296875, 0.5973493456840515, 0.1799178570508957, 0.761631429195404, 1.0430834293365479, 0.7305734157562256, 0.11638693511486053, 0.3129073977470398, 0.07584576308727264, 0.14698243141174316, 0.210170179605484, 0.05685615539550781, 0.1744699478149414, 0.19799408316612244, 0.055177975445985794, 0.2004132717847824, 0.15952901542186737, 0.0679096207022667, 0.29660362005233765, 0.0587344653904438, 0.7211635708808899, 0.8572263717651367, 0.41395440697669983, 0.5506079196929932, 0.8530203104019165, 0.5596153140068054, 0.2819426655769348, 0.4750007688999176, 0.1255989521741867, 0.7175959944725037, 0.8972499966621399, 0.49336615204811096, 0.43559402227401733, 0.7061159610748291, 0.3841596245765686, 0.49369120597839355, 0.7124460339546204, 0.3875676989555359, 0.45343533158302307, 0.6161104440689087, 0.19709238409996033, 0.7453451752662659, 1.028144121170044, 0.7173795104026794, 0.12748947739601135, 0.3226549029350281, 0.06680798530578613, 0.153657004237175, 0.202792689204216, 0.06012458726763725, 0.1684587001800537, 0.20312491059303284, 0.053049277514219284, 0.12203540652990341, 0.30886006355285645, 0.13141265511512756, 0.5731701850891113, 0.6413389444351196, 0.141858771443367, 0.8606573939323425, 1.1931560039520264, 0.9269550442695618, 0.12202439457178116, 1.1676654815673828, 1.7630136013031006, 1.7335631847381592, 1.1418933868408203, 0.06640362739562988, 1.3840014934539795, 2.1040232181549072, 2.18668270111084, 1.6958163976669312, 0.6888718605041504, 0.7824538946151733, 1.5414177179336548, 1.6593631505966187, 1.2004905939102173, 0.2225758582353592, 1.2223889827728271, 1.9578826427459717, 2.0549533367156982, 1.577540636062622, 0.60979825258255, 0.8551349639892578, 1.584817886352539, 1.6768295764923096, 1.1950321197509766, 0.1968938410282135, 1.2658679485321045, 2.0177810192108154, 2.1300272941589355, 1.6666663885116577, 0.6960688829421997, 0.7408714294433594, 1.4600636959075928, 1.543029546737671, 1.0534788370132446, 0.050954531878232956, 1.2163636684417725, 1.7902815341949463, 1.7422199249267578, 1.1344672441482544, 0.023076247423887253, 1.5415090322494507, 2.385161876678467, 2.5800719261169434, 2.191201686859131, 1.2770130634307861, 0.1098841205239296, 0.7938473224639893, 0.845260500907898, 0.3274635374546051, 0.7028596997261047, 1.0868219137191772, 0.8686510324478149, 0.1331566870212555, 1.0771995782852173, 1.580084204673767, 1.4686534404754639, 0.8044211268424988, 0.357255220413208, 0.8388112783432007, 0.708344042301178, 0.05643167346715927, 0.45583224296569824, 0.3464139997959137, 0.316100537776947, 0.34789377450942993, 0.18787693977355957, 0.1056855171918869, 0.5357619524002075, 0.5671939849853516, 0.09005169570446014, 0.7715978026390076, 0.9456180334091187, 0.5375602841377258, 0.3942885994911194, 0.6682974100112915, 0.3503364026546478, 0.5186964273452759, 0.7346739768981934, 0.404111385345459, 0.4307711124420166, 0.5954900979995728, 0.17915788292884827, 0.7600424289703369, 1.0407671928405762, 0.7289478182792664, 0.11608071625232697, 0.3121856153011322, 0.07565741240978241, 0.1466168910264969, 0.2096620500087738, 0.056557416915893555, 0.17411236464977264, 0.19740614295005798, 0.055353544652462006, 0.19973239302635193, 0.15939965844154358, 0.06762075424194336, 0.295980304479599, 0.05861864238977432, 0.7195305824279785, 0.8553215265274048, 0.4130694270133972, 0.5493441224098206, 0.8510767221450806, 0.5582781434059143, 0.28142523765563965, 0.47407254576683044, 0.12525658309459686, 0.7160554528236389, 0.8954375982284546, 0.49250927567481995, 0.4344255030155182, 0.7043135762214661, 0.3829422891139984, 0.4932459890842438]\n",
            "Loss Magnitude when epcohs is 1000: 0.4932459890842438\n",
            "Prediction: [[797.9862]]\n",
            "weight: [[3.1722627]] bias: [4.920514]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "No5xT7-X54P5",
        "outputId": "a9e76ab0-e7db-4610-9462-6cf26be86cec"
      },
      "source": [
        "#Pridicting X=250 using Adam optimizer, Mean Absolute Error, Epochs = 1000\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "x = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
        "y = [5, 8, 11, 14, 17, 20, 23, 26, 29, 32]\n",
        "# Define layer\n",
        "layer0 = tf.keras.layers.Dense(units=1, input_shape=[1])\n",
        "model = tf.keras.Sequential([layer0])\n",
        "# Compile model\n",
        "model.compile(loss='mean_absolute_error',\n",
        " optimizer=tf.keras.optimizers.Adam(1))\n",
        "# Train the model\n",
        "history = model.fit(x, y, epochs=1000, verbose=False)\n",
        "Expect = history.history ['loss']\n",
        "print (history.history ['loss'])\n",
        "print('Loss Magnitude when epcohs is 1000:', Expect [999])\n",
        "# Prediction\n",
        "print('Prediction: {}'.format(model.predict([250])))\n",
        "# Get weight and bias\n",
        "weights = layer0.get_weights()\n",
        "print('weight: {} bias: {}'.format(weights[0], weights[1]))"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[18.50248146057129, 13.002487182617188, 7.502511024475098, 2.002497673034668, 3.697598934173584, 6.591371059417725, 7.757240295410156, 7.7174482345581055, 6.699424743652344, 5.009115695953369, 2.7728946208953857, 0.5020832419395447, 2.7602028846740723, 4.265078067779541, 4.8157501220703125, 4.566726207733154, 3.641380786895752, 2.1396660804748535, 0.14356879889965057, 2.2789433002471924, 3.7706007957458496, 4.451920986175537, 4.425238132476807, 3.7779903411865234, 2.585301160812378, 0.9120287895202637, 1.1905829906463623, 2.482846736907959, 3.0452561378479004, 2.9553287029266357, 2.2855405807495117, 1.1144148111343384, 0.525751531124115, 1.4007055759429932, 1.6057713031768799, 1.212449312210083, 0.2842530608177185, 1.1245386600494385, 1.8351589441299438, 1.903770089149475, 1.395311713218689, 0.3835555911064148, 1.0992071628570557, 1.8516387939453125, 1.961316704750061, 1.494260549545288, 0.5095890164375305, 0.9397271275520325, 1.6790523529052734, 1.780745267868042, 1.3097054958343506, 0.32416003942489624, 1.1236330270767212, 1.8645846843719482, 1.9702829122543335, 1.5050170421600342, 0.5311697125434875, 0.8932584524154663, 1.6106293201446533, 1.6964648962020874, 1.2144654989242554, 0.2218860685825348, 1.2298059463500977, 1.9772907495498657, 2.0914559364318848, 1.636037826538086, 0.6683449745178223, 0.7600878477096558, 1.4876903295516968, 1.5848945379257202, 1.115046739578247, 0.1351209580898285, 1.3036344051361084, 2.041309356689453, 2.1482839584350586, 1.6878715753555298, 0.7170241475105286, 0.7130199670791626, 1.4434741735458374, 1.5445181131362915, 1.079291582107544, 0.1045958548784256, 1.3284616470336914, 2.0621283054351807, 2.1665263175964355, 1.7047488689422607, 0.733562171459198, 0.6959566473960876, 1.4268583059310913, 1.529160499572754, 1.0658648014068604, 0.09365983307361603, 1.3364455699920654, 2.0682332515716553, 2.1716690063476562, 1.709713339805603, 0.7390170097351074, 0.6894416809082031, 1.4200584888458252, 1.5227375030517578, 1.0603798627853394, 0.08959116786718369, 1.3386952877044678, 2.0694313049316406, 2.172478199005127, 1.7107021808624268, 0.7406750321388245, 0.6866946816444397, 1.4168506860733032, 1.5196107625961304, 1.0578010082244873, 0.08795986324548721, 1.3390347957611084, 2.069073438644409, 2.1719393730163574, 1.71042799949646, 0.741052508354187, 0.6853336095809937, 1.415024757385254, 1.517770528793335, 1.056337594985962, 0.08720970153808594, 1.3387794494628906, 2.068295955657959, 2.1710596084594727, 1.7098127603530884, 0.7410170435905457, 0.6845134496688843, 1.4137849807739258, 1.5164902210235596, 1.0553457736968994, 0.08679547160863876, 1.3383686542510986, 2.067462921142578, 2.170156955718994, 1.7091493606567383, 0.7408602833747864, 0.683931291103363, 1.412834882736206, 1.5154950618743896, 1.054587960243225, 0.08652110397815704, 1.3379446268081665, 2.0666868686676025, 2.1693286895751953, 1.708531141281128, 0.7406826019287109, 0.6834689378738403, 1.4120508432388306, 1.5146684646606445, 1.0539638996124268, 0.08631134033203125, 1.3375542163848877, 2.0659914016723633, 2.168590784072876, 1.7079766988754272, 0.740511953830719, 0.6830815076828003, 1.4113813638687134, 1.5139602422714233, 1.0534299612045288, 0.08613839000463486, 1.3372024297714233, 2.065375566482544, 2.1679375171661377, 1.707486867904663, 0.7403610944747925, 0.6827415227890015, 1.4107941389083862, 1.5133388042449951, 1.0529619455337524, 0.08598947525024414, 1.336889386177063, 2.0648276805877686, 2.1673572063446045, 1.7070496082305908, 0.7402210831642151, 0.682447075843811, 1.4102786779403687, 1.5127928256988525, 1.0525522232055664, 0.08586087077856064, 1.336607575416565, 2.0643393993377686, 2.166839361190796, 1.7066612243652344, 0.7400994300842285, 0.6821814775466919, 1.4098182916641235, 1.5123041868209839, 1.0521844625473022, 0.08574481308460236, 1.33635675907135, 2.0639021396636963, 2.1663761138916016, 1.706312894821167, 0.7399899363517761, 0.6819424629211426, 1.4094032049179077, 1.5118656158447266, 1.0518553256988525, 0.08564295619726181, 1.3361295461654663, 2.063508987426758, 2.165959358215332, 1.7059991359710693, 0.7398918271064758, 0.6817274689674377, 1.40902841091156, 1.511468529701233, 1.0515563488006592, 0.0855487808585167, 1.3359251022338867, 2.0631518363952637, 2.1655819416046143, 1.7057154178619385, 0.7398033142089844, 0.681531548500061, 1.408689260482788, 1.5111095905303955, 1.0512864589691162, 0.0854637622833252, 1.3357388973236084, 2.0628292560577393, 2.16524076461792, 1.7054598331451416, 0.7397235631942749, 0.6813521385192871, 1.4083781242370605, 1.5107805728912354, 1.051038384437561, 0.0853852778673172, 1.3355709314346313, 2.0625357627868652, 2.164929151535034, 1.7052249908447266, 0.7396522760391235, 0.6811884641647339, 1.4080947637557983, 1.5104795694351196, 1.0508129596710205, 0.08531560748815536, 1.335413932800293, 2.0622642040252686, 2.1646409034729004, 1.705008864402771, 0.7395839095115662, 0.6810393929481506, 1.4078359603881836, 1.5102052688598633, 1.0506057739257812, 0.08525080978870392, 1.3352694511413574, 2.062014102935791, 2.1643757820129395, 1.704810380935669, 0.7395231127738953, 0.680899441242218, 1.4075944423675537, 1.509948968887329, 1.0504138469696045, 0.08519134670495987, 1.3351367712020874, 2.0617835521698, 2.164132595062256, 1.7046273946762085, 0.7394665479660034, 0.6807719469070435, 1.4073740243911743, 1.5097160339355469, 1.0502389669418335, 0.0851379856467247, 1.3350125551223755, 2.0615694522857666, 2.163905382156372, 1.704456090927124, 0.7394127249717712, 0.6806544065475464, 1.4071686267852783, 1.5094988346099854, 1.050075650215149, 0.08508691936731339, 1.3348979949951172, 2.061370611190796, 2.1636948585510254, 1.7042980194091797, 0.7393633127212524, 0.6805444359779358, 1.406977891921997, 1.5092966556549072, 1.0499241352081299, 0.08504047244787216, 1.334791898727417, 2.0611865520477295, 2.1635005474090576, 1.7041515111923218, 0.739317774772644, 0.6804413199424744, 1.4067987203598022, 1.5091071128845215, 1.049782156944275, 0.0849975124001503, 1.3346920013427734, 2.061013698577881, 2.163317918777466, 1.7040153741836548, 0.7392764091491699, 0.6803441047668457, 1.406632423400879, 1.5089296102523804, 1.0496495962142944, 0.08495607227087021, 1.334599256515503, 2.0608534812927246, 2.16314697265625, 1.7038856744766235, 0.7392366528511047, 0.6802542209625244, 1.4064756631851196, 1.5087655782699585, 1.0495251417160034, 0.08491792529821396, 1.33451247215271, 2.0607028007507324, 2.1629881858825684, 1.7037668228149414, 0.7391998767852783, 0.6801695823669434, 1.4063291549682617, 1.508609652519226, 1.049408197402954, 0.08487997204065323, 1.3344316482543945, 2.0605618953704834, 2.1628384590148926, 1.7036552429199219, 0.7391659617424011, 0.6800892949104309, 1.4061906337738037, 1.5084642171859741, 1.0492995977401733, 0.0848478302359581, 1.3343532085418701, 2.060426950454712, 2.1626954078674316, 1.7035481929779053, 0.7391326427459717, 0.6800157427787781, 1.4060633182525635, 1.508327841758728, 1.049196481704712, 0.08481641113758087, 1.3342798948287964, 2.0603013038635254, 2.162562370300293, 1.703447699546814, 0.7391018867492676, 0.6799446940422058, 1.4059405326843262, 1.5081979036331177, 1.049098014831543, 0.08478617668151855, 1.3342119455337524, 2.060183048248291, 2.1624369621276855, 1.7033535242080688, 0.7390731573104858, 0.6798779964447021, 1.4058244228363037, 1.5080745220184326, 1.0490058660507202, 0.08475665748119354, 1.3341491222381592, 2.060072898864746, 2.162320375442505, 1.7032657861709595, 0.7390472888946533, 0.679813802242279, 1.4057140350341797, 1.507958173751831, 1.0489184856414795, 0.08472990989685059, 1.3340864181518555, 2.0599656105041504, 2.1622068881988525, 1.7031805515289307, 0.7390205264091492, 0.6797531247138977, 1.4056098461151123, 1.507847547531128, 1.0488353967666626, 0.08470387756824493, 1.334028959274292, 2.0598654747009277, 2.1621005535125732, 1.7031008005142212, 0.738996684551239, 0.6796966791152954, 1.4055118560791016, 1.5077450275421143, 1.0487589836120605, 0.08468194305896759, 1.3339725732803345, 2.0597691535949707, 2.1619982719421387, 1.7030236721038818, 0.738973081111908, 0.6796432733535767, 1.405419945716858, 1.5076462030410767, 1.0486843585968018, 0.0846579521894455, 1.333919882774353, 2.0596776008605957, 2.1619014739990234, 1.7029516696929932, 0.738950788974762, 0.6795917749404907, 1.4053304195404053, 1.5075515508651733, 1.0486133098602295, 0.08463587611913681, 1.3338702917099, 2.0595908164978027, 2.1618096828460693, 1.7028827667236328, 0.7389298677444458, 0.6795423626899719, 1.4052460193634033, 1.5074622631072998, 1.048546552658081, 0.08461685478687286, 1.3338221311569214, 2.0595078468322754, 2.161721706390381, 1.7028169631958008, 0.7389087677001953, 0.6794971227645874, 1.4051661491394043, 1.50737726688385, 1.0484836101531982, 0.08459778130054474, 1.3337745666503906, 2.059427261352539, 2.1616368293762207, 1.702752709388733, 0.7388889789581299, 0.6794528961181641, 1.4050899744033813, 1.5072977542877197, 1.0484228134155273, 0.08458013832569122, 1.333730936050415, 2.0593514442443848, 2.1615567207336426, 1.7026910781860352, 0.7388683557510376, 0.6794122457504272, 1.4050180912017822, 1.5072205066680908, 1.0483648777008057, 0.0845622569322586, 1.3336896896362305, 2.059279680252075, 2.161480188369751, 1.7026342153549194, 0.738851010799408, 0.67937171459198, 1.4049487113952637, 1.5071470737457275, 1.0483102798461914, 0.0845465213060379, 1.333648681640625, 2.0592098236083984, 2.161407470703125, 1.702580213546753, 0.7388355135917664, 0.6793311834335327, 1.404878854751587, 1.5070737600326538, 1.0482555627822876, 0.08452925831079483, 1.3336106538772583, 2.0591442584991455, 2.16133713722229, 1.7025268077850342, 0.7388190627098083, 0.6792944073677063, 1.4048141241073608, 1.5070050954818726, 1.0482038259506226, 0.08451304584741592, 1.3335739374160767, 2.059081554412842, 2.1612703800201416, 1.7024762630462646, 0.738802433013916, 0.6792598962783813, 1.404754400253296, 1.5069414377212524, 1.048156976699829, 0.08450045436620712, 1.3335374593734741, 2.059018611907959, 2.1612040996551514, 1.7024269104003906, 0.7387868165969849, 0.6792267560958862, 1.404695749282837, 1.506879448890686, 1.0481088161468506, 0.08448486030101776, 1.3335040807724, 2.0589616298675537, 2.1611435413360596, 1.7023818492889404, 0.7387741804122925, 0.6791917085647583, 1.404637336730957, 1.5068180561065674, 1.0480637550354004, 0.08447284996509552, 1.333470106124878, 2.0589041709899902, 2.1610825061798096, 1.7023351192474365, 0.7387588024139404, 0.6791619658470154, 1.4045841693878174, 1.506760835647583, 1.0480210781097412, 0.08445854485034943, 1.3334394693374634, 2.058850049972534, 2.1610264778137207, 1.7022931575775146, 0.7387464046478271, 0.6791306734085083, 1.4045313596725464, 1.506705403327942, 1.0479801893234253, 0.08444757759571075, 1.3334081172943115, 2.0587968826293945, 2.1609690189361572, 1.7022502422332764, 0.7387315630912781, 0.6791033744812012, 1.4044817686080933, 1.506653070449829, 1.0479402542114258, 0.08443622291088104, 1.3333780765533447, 2.058746337890625, 2.1609153747558594, 1.70220947265625, 0.7387175559997559, 0.6790776252746582, 1.4044358730316162, 1.5066044330596924, 1.0479053258895874, 0.08442731201648712, 1.3333470821380615, 2.0586955547332764, 2.1608619689941406, 1.7021690607070923, 0.7387049198150635, 0.679050624370575, 1.404388189315796, 1.506552815437317, 1.047865867614746, 0.0844142884016037, 1.333322286605835, 2.058650016784668, 2.1608145236968994, 1.702134132385254, 0.7386953830718994, 0.6790223717689514, 1.4043413400650024, 1.5065042972564697, 1.0478285551071167, 0.0844026580452919, 1.333296537399292, 2.058605670928955, 2.1607677936553955, 1.7020986080169678, 0.7386848330497742, 0.678997278213501, 1.4042975902557373, 1.5064576864242554, 1.0477933883666992, 0.08439178764820099, 1.3332725763320923, 2.058563709259033, 2.16072154045105, 1.702063798904419, 0.738673985004425, 0.6789731383323669, 1.4042553901672363, 1.5064136981964111, 1.0477612018585205, 0.08438277244567871, 1.333247423171997, 2.058521270751953, 2.160677433013916, 1.7020301818847656, 0.7386630773544312, 0.6789501309394836, 1.4042166471481323, 1.506371021270752, 1.0477302074432373, 0.08437476307153702, 1.3332220315933228, 2.058479070663452, 2.160632371902466, 1.7019962072372437, 0.7386513948440552, 0.6789295077323914, 1.4041783809661865, 1.5063310861587524, 1.0477004051208496, 0.08436641842126846, 1.3331977128982544, 2.0584378242492676, 2.1605889797210693, 1.7019634246826172, 0.7386391758918762, 0.6789091229438782, 1.4041415452957153, 1.506292462348938, 1.0476710796356201, 0.08435878902673721, 1.3331756591796875, 2.0583996772766113, 2.1605494022369385, 1.7019336223602295, 0.738630473613739, 0.6788881421089172, 1.404105305671692, 1.5062528848648071, 1.0476417541503906, 0.08435006439685822, 1.3331530094146729, 2.0583627223968506, 2.160510301589966, 1.701904296875, 0.7386223077774048, 0.6788669228553772, 1.404068946838379, 1.5062153339385986, 1.0476127862930298, 0.08434081077575684, 1.3331329822540283, 2.058328628540039, 2.1604745388031006, 1.7018768787384033, 0.7386142611503601, 0.6788461208343506, 1.4040330648422241, 1.5061771869659424, 1.0475842952728271, 0.08433131873607635, 1.3331153392791748, 2.0582962036132812, 2.1604390144348145, 1.7018508911132812, 0.7386062741279602, 0.6788257360458374, 1.4039984941482544, 1.5061396360397339, 1.0475561618804932, 0.08432126045227051, 1.33309805393219, 2.0582642555236816, 2.1604058742523193, 1.7018260955810547, 0.7386007905006409, 0.678804337978363, 1.4039647579193115, 1.5061051845550537, 1.0475304126739502, 0.0843137726187706, 1.3330795764923096, 2.058232069015503, 2.1603708267211914, 1.7018007040023804, 0.7385926842689514, 0.6787868738174438, 1.403932809829712, 1.5060714483261108, 1.047504186630249, 0.0843057632446289, 1.3330614566802979, 2.058202028274536, 2.1603400707244873, 1.7017772197723389, 0.738586962223053, 0.6787670254707336, 1.4039011001586914, 1.5060373544692993, 1.0474798679351807, 0.08429674804210663, 1.3330447673797607, 2.058171272277832, 2.1603074073791504, 1.7017529010772705, 0.7385788559913635, 0.6787508130073547, 1.403870940208435, 1.5060052871704102, 1.0474547147750854, 0.0842888355255127, 1.33302903175354, 2.0581440925598145, 2.160277843475342, 1.7017314434051514, 0.7385731339454651, 0.6787332892417908, 1.4038407802581787, 1.5059752464294434, 1.0474315881729126, 0.08428172767162323, 1.3330119848251343, 2.0581154823303223, 2.160247802734375, 1.7017085552215576, 0.7385665774345398, 0.6787163019180298, 1.4038130044937134, 1.5059446096420288, 1.0474096536636353, 0.08427491039037704, 1.3329964876174927, 2.0580875873565674, 2.1602187156677246, 1.7016868591308594, 0.7385604977607727, 0.6786991357803345, 1.4037845134735107, 1.5059140920639038, 1.0473852157592773, 0.08426646888256073, 1.3329832553863525, 2.058062791824341, 2.1601920127868652, 1.701667070388794, 0.7385555505752563, 0.6786822080612183, 1.4037573337554932, 1.5058858394622803, 1.0473639965057373, 0.0842597484588623, 1.3329684734344482, 2.058037042617798, 2.1601648330688477, 1.7016464471817017, 0.7385491132736206, 0.6786680221557617, 1.4037317037582397, 1.5058584213256836, 1.0473430156707764, 0.0842529758810997, 1.3329541683197021, 2.0580124855041504, 2.1601388454437256, 1.7016279697418213, 0.738544762134552, 0.6786516904830933, 1.403705358505249, 1.5058302879333496, 1.0473220348358154, 0.08424524962902069, 1.3329401016235352, 2.057988166809082, 2.1601130962371826, 1.7016074657440186, 0.7385382056236267, 0.6786385178565979, 1.4036809206008911, 1.5058046579360962, 1.047303557395935, 0.0842411071062088, 1.3329261541366577, 2.0579631328582764, 2.1600863933563232, 1.7015876770019531, 0.7385320067405701, 0.6786255240440369, 1.40365731716156, 1.5057804584503174, 1.0472841262817383, 0.08423466980457306, 1.3329133987426758, 2.057940721511841, 2.160062789916992, 1.7015712261199951, 0.7385280132293701, 0.6786102056503296, 1.4036333560943604, 1.5057542324066162, 1.0472657680511475, 0.08422908931970596, 1.332899808883667, 2.0579185485839844, 2.1600379943847656, 1.7015517950057983, 0.7385209798812866, 0.6785988807678223, 1.4036118984222412, 1.505732536315918, 1.0472491979599, 0.0842248946428299, 1.3328851461410522, 2.057894229888916, 2.1600136756896973, 1.701533555984497, 0.73851478099823, 0.6785858869552612, 1.4035905599594116, 1.5057084560394287, 1.0472310781478882, 0.08421897888183594, 1.3328728675842285, 2.057873487472534, 2.159990072250366, 1.701515555381775, 0.7385093569755554, 0.6785748600959778, 1.4035698175430298, 1.5056873559951782, 1.0472145080566406, 0.08421464264392853, 1.3328608274459839, 2.0578510761260986, 2.1599678993225098, 1.7014997005462646, 0.7385059595108032, 0.6785607933998108, 1.403548240661621, 1.505663514137268, 1.047197937965393, 0.08420886844396591, 1.3328492641448975, 2.057831287384033, 2.159947395324707, 1.701483964920044, 0.7385002970695496, 0.6785492300987244, 1.4035272598266602, 1.505642294883728, 1.047181248664856, 0.0842040553689003, 1.3328379392623901, 2.0578112602233887, 2.159925937652588, 1.7014678716659546, 0.7384957671165466, 0.678537905216217, 1.4035072326660156, 1.5056207180023193, 1.0471649169921875, 0.08419790118932724, 1.3328275680541992, 2.0577940940856934, 2.1599063873291016, 1.7014535665512085, 0.7384923696517944, 0.6785253286361694, 1.403486967086792, 1.505599856376648, 1.0471484661102295, 0.08419284969568253, 1.3328173160552979, 2.057774305343628, 2.1598873138427734, 1.701438546180725, 0.7384878396987915, 0.6785138845443726, 1.4034678936004639, 1.5055789947509766, 1.0471328496932983, 0.08418688923120499, 1.332808017730713, 2.057757616043091, 2.159868001937866, 1.7014238834381104, 0.7384831309318542, 0.6785040497779846, 1.403450846672058, 1.505561351776123, 1.0471208095550537, 0.08418607711791992, 1.332793951034546, 2.0577361583709717, 2.159846067428589, 1.7014074325561523, 0.7384775280952454, 0.6784954071044922, 1.4034335613250732, 1.5055434703826904, 1.0471079349517822, 0.08418288081884384, 1.3327820301055908, 2.0577168464660645, 2.159825563430786, 1.701392412185669, 0.738472580909729, 0.678485095500946, 1.4034160375595093, 1.5055242776870728, 1.0470927953720093, 0.08417711406946182, 1.3327727317810059, 2.0577003955841064, 2.1598081588745117, 1.7013797760009766, 0.7384697198867798, 0.6784738302230835, 1.4033976793289185, 1.5055052042007446, 1.0470778942108154, 0.08417177200317383, 1.3327646255493164, 2.0576858520507812, 2.1597936153411865, 1.7013683319091797, 0.7384663224220276, 0.678463339805603, 1.4033806324005127, 1.5054856538772583, 1.0470637083053589, 0.08416686207056046, 1.3327566385269165, 2.0576703548431396, 2.159776449203491, 1.7013565301895142, 0.7384637594223022, 0.6784520149230957, 1.403362512588501, 1.505467414855957, 1.0470497608184814, 0.08416175842285156, 1.3327481746673584, 2.057654857635498, 2.159759998321533, 1.701343297958374, 0.7384604215621948, 0.6784420013427734, 1.403344750404358, 1.505448579788208, 1.0470348596572876, 0.08415527641773224, 1.3327423334121704, 2.0576415061950684, 2.159745216369629, 1.7013332843780518, 0.738458514213562, 0.6784318089485168, 1.4033282995224, 1.5054315328598022, 1.0470223426818848, 0.08415236324071884, 1.3327314853668213, 2.0576252937316895, 2.1597282886505127, 1.701321005821228, 0.738455057144165, 0.6784223914146423, 1.4033128023147583, 1.5054149627685547, 1.0470092296600342, 0.08414831012487411, 1.3327237367630005, 2.057610034942627, 2.1597132682800293, 1.7013088464736938, 0.7384508848190308, 0.6784151196479797, 1.40329909324646, 1.5054008960723877, 1.0469986200332642, 0.08414492756128311, 1.3327147960662842, 2.05759596824646, 2.1596970558166504, 1.701297402381897, 0.7384480834007263, 0.6784065961837769, 1.4032841920852661, 1.505385160446167, 1.046987771987915, 0.0841430202126503, 1.3327043056488037, 2.057579278945923, 2.159679889678955, 1.7012830972671509, 0.7384411096572876, 0.678400456905365, 1.403272032737732, 1.5053722858428955]\n",
            "Loss Magnitude when epcohs is 1000: 1.5053722858428955\n",
            "Prediction: [[708.1993]]\n",
            "weight: [[2.8136303]] bias: [4.7916856]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wxq2GxZc_g8K"
      },
      "source": [
        "Coding without loss magnitude\n",
        "\n",
        "\n",
        "*   higher epoch giving lower prediction value \n",
        "*   List item\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CZhDRg_d_YrT",
        "outputId": "1d171649-da11-47b2-8208-1936f8de0228"
      },
      "source": [
        "#Pridicting X=250 using Adam optimizer but with Mean Absolute Error Epoch:1000\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "x = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
        "y = [5, 8, 11, 14, 17, 20, 23, 26, 29, 32]\n",
        "# Define layer\n",
        "layer0 = tf.keras.layers.Dense(units=1, input_shape=[1])\n",
        "model = tf.keras.Sequential([layer0])\n",
        "# Compile model\n",
        "model.compile(loss='mean_absolute_error',\n",
        " optimizer=tf.keras.optimizers.Adam(1))\n",
        "# Train the model\n",
        "history = model.fit(x, y, epochs=1000, verbose=False)\n",
        "# Prediction\n",
        "print('Prediction: {}'.format(model.predict([250])))\n",
        "# Get weight and bias\n",
        "weights = layer0.get_weights()\n",
        "print('weight: {} bias: {}'.format(weights[0], weights[1]))"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction: [[742.93787]]\n",
            "weight: [[2.9521472]] bias: [4.9010797]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nxY5YBhf_pTa",
        "outputId": "4ed53284-e560-453c-9baf-e8add1cab549"
      },
      "source": [
        "#Pridicting X=250 using Adam optimizer but with Mean Absolute Error Epoch 100\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "x = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
        "y = [5, 8, 11, 14, 17, 20, 23, 26, 29, 32]\n",
        "# Define layer\n",
        "layer0 = tf.keras.layers.Dense(units=1, input_shape=[1])\n",
        "model = tf.keras.Sequential([layer0])\n",
        "# Compile model\n",
        "model.compile(loss='mean_absolute_error',\n",
        " optimizer=tf.keras.optimizers.Adam(1))\n",
        "# Train the model\n",
        "history = model.fit(x, y, epochs=100, verbose=False)\n",
        "# Prediction\n",
        "print('Prediction: {}'.format(model.predict([250])))\n",
        "# Get weight and bias\n",
        "weights = layer0.get_weights()\n",
        "print('weight: {} bias: {}'.format(weights[0], weights[1]))"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction: [[779.4166]]\n",
            "weight: [[3.097249]] bias: [5.1043773]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXUboPXe0h5d"
      },
      "source": [
        "\n",
        "# Mean Square Error Vs Mean Absolute Error "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iC68lzfm11fj"
      },
      "source": [
        "\n",
        "\n",
        "*   MSE resulting higher value, but with lower bias \n",
        "*   MAE resulting lower value(Prediction), with higher bias\n",
        "*   Weight almost the same with +/- 0.1\n",
        "\n",
        "\n",
        "\n",
        "*   UPDATE: weight sama, MSE bias higher\n",
        "*   UPDATE: MSE & MAE could have same weight and bias\n",
        "*   UPDATE: MSE always higher pridiction value\n",
        "*   \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "imB5gbkk1tQQ",
        "outputId": "cb496fab-6d91-4d9c-85b2-eb737072fdaf"
      },
      "source": [
        "#Pridicting X=250 using Adam optimizer but with Mean Square Error\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "x = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
        "y = [5, 8, 11, 14, 17, 20, 23, 26, 29, 32]\n",
        "# Define layer\n",
        "layer0 = tf.keras.layers.Dense(units=1, input_shape=[1])\n",
        "model = tf.keras.Sequential([layer0])\n",
        "# Compile model\n",
        "model.compile(loss='mean_squared_error',\n",
        " optimizer=tf.keras.optimizers.Adam(1))\n",
        "# Train the model\n",
        "history = model.fit(x, y, epochs=100, verbose=False)\n",
        "# Prediction\n",
        "print('Prediction: {}'.format(model.predict([250])))\n",
        "# Get weight and bias\n",
        "weights = layer0.get_weights()\n",
        "print('weight: {} bias: {}'.format(weights[0], weights[1]))\n"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction: [[754.57947]]\n",
            "weight: [[2.998302]] bias: [5.0039415]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A3B8cS-v0x86",
        "outputId": "5d3d8eb7-1486-4410-a7d1-7c5cad1cb987"
      },
      "source": [
        "#Pridicting X=250 using Adam optimizer but with Mean Absolute Error\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "x = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
        "y = [5, 8, 11, 14, 17, 20, 23, 26, 29, 32]\n",
        "# Define layer\n",
        "layer0 = tf.keras.layers.Dense(units=1, input_shape=[1])\n",
        "model = tf.keras.Sequential([layer0])\n",
        "# Compile model\n",
        "model.compile(loss='mean_absolute_error',\n",
        " optimizer=tf.keras.optimizers.Adam(1))\n",
        "# Train the model\n",
        "history = model.fit(x, y, epochs=100, verbose=False)\n",
        "# Prediction\n",
        "print('Prediction: {}'.format(model.predict([250])))\n",
        "# Get weight and bias\n",
        "weights = layer0.get_weights()\n",
        "print('weight: {} bias: {}'.format(weights[0], weights[1]))\n"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction: [[734.66113]]\n",
            "weight: [[2.9191499]] bias: [4.8736873]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AagTNQ_OkI1J"
      },
      "source": [
        "# Difference Optimizer Testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "St_jmrQgjVtZ"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "*   Value change everytime refreshed\n",
        "*   Something wrong Adadelta out of range (compare to other 7++)\n",
        "*   SGD required optimizer=tf.keras.optimizers.SGD(momentum=0.9) *define momentum, usually 0.9\n",
        "*   Somehow bigger value doesnt mean more accurate\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gb3yPBw7ikAT",
        "outputId": "6d8d7127-3c6c-4c82-934e-346f518ec238"
      },
      "source": [
        "#using optimizer: Adagrad\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "x = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
        "y = [5, 8, 11, 14, 17, 20, 23, 26, 29, 32]\n",
        "# Define layer\n",
        "layer0 = tf.keras.layers.Dense(units=1, input_shape=[1])\n",
        "model = tf.keras.Sequential([layer0])\n",
        "# Compile model\n",
        "model.compile(loss='mean_squared_error',\n",
        " optimizer=tf.keras.optimizers.Adagrad(1))\n",
        "# Train the model\n",
        "history = model.fit(x, y, epochs=100, verbose=False)\n",
        "# Prediction\n",
        "print('Prediction: {}'.format(model.predict([250])))\n",
        "# Get weight and bias\n",
        "weights = layer0.get_weights()\n",
        "print('weight: {} bias: {}'.format(weights[0], weights[1]))\n"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction: [[774.80115]]\n",
            "weight: [[3.0811546]] bias: [4.512535]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OzGTjG8jjjRj",
        "outputId": "f137a37d-1d62-4636-8a6c-19298925556a"
      },
      "source": [
        "#using optimizer: Adam\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "x = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
        "y = [5, 8, 11, 14, 17, 20, 23, 26, 29, 32]\n",
        "# Define layer\n",
        "layer0 = tf.keras.layers.Dense(units=1, input_shape=[1])\n",
        "model = tf.keras.Sequential([layer0])\n",
        "# Compile model\n",
        "model.compile(loss='mean_squared_error',\n",
        " optimizer=tf.keras.optimizers.Adam(1))\n",
        "# Train the model\n",
        "history = model.fit(x, y, epochs=100, verbose=False)\n",
        "# Prediction\n",
        "print('Prediction: {}'.format(model.predict([250])))\n",
        "# Get weight and bias\n",
        "weights = layer0.get_weights()\n",
        "print('weight: {} bias: {}'.format(weights[0], weights[1]))\n"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction: [[752.1973]]\n",
            "weight: [[2.9888618]] bias: [4.981897]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4MT687R5lDfB",
        "outputId": "ff6601e2-2360-4d0e-cad0-f3d2a683b7ea"
      },
      "source": [
        "#using optimizer: SGD + momentum\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "x = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
        "y = [5, 8, 11, 14, 17, 20, 23, 26, 29, 32]\n",
        "# Define layer\n",
        "layer0 = tf.keras.layers.Dense(units=1, input_shape=[1])\n",
        "model = tf.keras.Sequential([layer0])\n",
        "# Compile model\n",
        "model.compile(loss='mean_squared_error',\n",
        " optimizer=tf.keras.optimizers.SGD(momentum=0.9))    \n",
        "# Train the model\n",
        "history = model.fit(x, y, epochs=100, verbose=False)\n",
        "# Prediction\n",
        "print('Prediction: {}'.format(model.predict([250])))\n",
        "# Get weight and bias\n",
        "weights = layer0.get_weights()\n",
        "print('weight: {} bias: {}'.format(weights[0], weights[1]))"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction: [[751.0105]]\n",
            "weight: [[2.984076]] bias: [4.991494]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YcsEjZFTfyUy",
        "outputId": "bc05fb5f-5c80-4f03-819a-8145cafe070e"
      },
      "source": [
        "#using optimizer: RMSprop\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "x = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
        "y = [5, 8, 11, 14, 17, 20, 23, 26, 29, 32]\n",
        "# Define layer\n",
        "layer0 = tf.keras.layers.Dense(units=1, input_shape=[1])\n",
        "model = tf.keras.Sequential([layer0])\n",
        "# Compile model\n",
        "model.compile(loss='mean_squared_error',\n",
        " optimizer=tf.keras.optimizers.RMSprop(1))\n",
        "# Train the model\n",
        "history = model.fit(x, y, epochs=100, verbose=False)\n",
        "# Prediction\n",
        "print('Prediction: {}'.format(model.predict([250])))\n",
        "# Get weight and bias\n",
        "weights = layer0.get_weights()\n",
        "print('weight: {} bias: {}'.format(weights[0], weights[1]))\n"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction: [[632.4815]]\n",
            "weight: [[2.5118797]] bias: [4.5115848]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0D_wmwSMi-0y",
        "outputId": "b1cdd019-f864-4e1c-f9a0-a609e549ebe0"
      },
      "source": [
        "#using optimizer: Adadelta\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "x = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
        "y = [5, 8, 11, 14, 17, 20, 23, 26, 29, 32]\n",
        "# Define layer\n",
        "layer0 = tf.keras.layers.Dense(units=1, input_shape=[1])\n",
        "model = tf.keras.Sequential([layer0])\n",
        "# Compile model\n",
        "model.compile(loss='mean_squared_error',\n",
        " optimizer=tf.keras.optimizers.Adadelta(1))\n",
        "# Train the model\n",
        "history = model.fit(x, y, epochs=100, verbose=False)\n",
        "# Prediction\n",
        "print('Prediction: {}'.format(model.predict([250])))\n",
        "# Get weight and bias\n",
        "weights = layer0.get_weights()\n",
        "print('weight: {} bias: {}'.format(weights[0], weights[1]))"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction: [[46.84556]]\n",
            "weight: [[0.18676624]] bias: [0.15399857]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}